\chapter{Linear Mapping}
\section{Mapping}

\section{Linear Mapping}

\begin{pro}%12
	Suppose $\varphi\colon F^{n\times n}\to F$ is a linear mapping, then for all $A,B\in F^{n\times n}$, 
	$\varphi(AB)=\varphi(BA)$ if and only if there is some $\la\in F$, such that $\varphi=\la\tr$.
\end{pro}
\begin{proof}
	Suppose $A=(a_{ij})$, $B=(b_{ij})$ and $\varphi=\la \tr$, then
	\begin{align*}
	\tr(AB)&=\sum_{i}\sum_{j}a_{ij}b_{ji}\\
		   &=\sum_{j}\sum_{i}b_{ji}a_{ij}\\
		   &=\tr(BA),
	\end{align*}
	thus $\varphi(AB)=\varphi(BA)$. Conversely, suppose $\varphi(AB)=\varphi(BA)$, Now for any $1\leq i\neq j\leq n$
	and for any $a\in F$, let	
	\[a_{ii}=a,\quad a_{st}=0\;\mbox{if}\;(s,t)\neq (i,i),\quad\mbox{and}\quad B=E_{ij},\]
	then one can prove $AB=aE_{ij}$ and $BA=0$, thus $\varphi(aE_{ij})=0$, i.e., we have shown that
	\[\varphi\begin{pmatrix}
	0 & & *\\
	  &\ddots& \\
	* & & 0\end{pmatrix}=0.\]
	Now let $\la=\varphi(E_{11})$, and $P_{12}$ is the elementary matrix whose function is to exchange the first and second rows of any matrix by left multiplication (and surely, to exchange the first and second columns by right multiplication), then
	\begin{align*}
		\varphi(E_{11})&=\varphi(E_{11}+E_{12})\\
					&=\varphi\big((E_{11}+E_{12})P_{12}\big)\\
					&=\varphi\big(P_{12}(E_{11}+E_{12})\big)\\
					&=\varphi(E_{21}+E_{22})\\
					&=\varphi(E_{22}).
	\end{align*}
	Similarly, one can prove that $\la=\varphi(E_{11})=\dots=\varphi(E_{nn})$. Now we know that 
	\[\varphi(A)=\sum_i a_{ii} \la=\la \tr(A),\quad(\forall A\in F^{n\times n}),\]
	i.e, $\varphi=\la\tr$.
\end{proof}

\section{Algebraic Operation of Linear Mapping}

\begin{pro}%8
	Suppose $V$ is an order $n$ vector space on number field $F$. All the linear mapping $\ma{A}\colon V\to V$ construct a new vector space called $U$. Given a fixed $\ma{A}\in U$, define $P_{\ma{A}}\colon U\to U$ as follows: for any $\ma{X}\in U$, let $P_{\ma{A}}(\ma{X})$. Also, we define $W$ as the space with all linear mapping from $U$ to $U$. Then
	\begin{description}
	\item[(a)] Show that $P_{\ma{A}}\in W$, i.e, it is a linear mapping.
	\item[(b)] Show that \emph{given any $\ma{D}\in W$, there is some $\ma{A}\in U$, such that $\ma{D}=P_{\ma{A}}$} if and
	only if $n=1$.
	\item[(c)] Given any $n\geq 1$, find an equivalent condition of $\ma{D}\in W$ such that there is some $\ma{A}\in U$, such that $\ma{D}=P_{\ma{A}}$.
	\end{description}
\end{pro}

\begin{proof}
	\begin{description}
	\item[(a)] Trivial.
	\item[(b)] We define $P\colon U\to W$ as $P(\ma{A})=P_{\ma{A}}$ for all $\ma{A}\in U$, then one can easily prove that $P$ is a linear mapping. Suppose $P(\ma{A})=0$, then for any $\ma{X}\in U$, $\ma{A}\ma{X}=0\in U$, we know $\ma{A}=0$ by its matrix under a basis of $U$. Thus $\Ker{P}=\{0\}$ and 
	\[U\cong\IM{P}<W.\]
	Since $\dim U=n^2$ and $\dim W=n^4$, we know that $\IM{P}\cong W$ if and only if $n=1$. In this case, $W\cong U\cong V\cong F$.
	\item[(c)] First we suppose $\{\al_1,\al_2,\dots,\al_n\}$ is a basis of $V$ and $\al=\sum_{i=1}^n a_i \al_i$, then if we define
	$\varepsilon_{ij}\in U$ for $1\leq i,j\leq n$ as
	\[\varepsilon_{ij}(\al)=a_i \al_j,\]
	one can easily show that $\{\varepsilon_{ij}\colon 1\leq i,j\leq n\}$ is a basis of $U$ and $I=\sum_{i=1}^n \varepsilon_{ii}$ is the identity mapping from $V$ to $V$. Moreover, we can multiply $\varepsilon_{ij}$ and get for all $1\leq i,j,k,\ell\leq n$ and for all $\al\in V$ with the form above, 
	\begin{align*}
	\varepsilon_{ij}\varepsilon_{k\ell}(\al)&=\varepsilon_{ij}(a_k \al_{\ell})\\
							&=\delta_{i\ell} a_k \al_j\\
							&=\delta_{i\ell} \varepsilon_{kj}(\al),
	\end{align*}
	or simply, $\varepsilon_{ij}\varepsilon_{k\ell}=\delta_{i\ell} \varepsilon_{kj}$. 

	Now assume that $\ma{D}\in W$ satisfies the condition, i.e, $\ma{D}=P_{\ma{A}}$ where $\ma{A}\in U$, then 
	$\ma{D}(I)=P_{\ma{A}}(I)=\ma{A}$, thus for all $\ma{X}\in U$,
	\[\ma{D}\ma{X}=\ma{D}(I)\ma{X}.\tag{1}\]
	We describe $\ma{D}$ by its matrix under the basis $\{\varepsilon_{ij}\colon 1\leq i,j\leq n\}$. Write that 
	\[\ma{D}(\varepsilon_{ij})=\sum_{k,\ell} d_{ij}^{k\ell} \varepsilon_{k\ell},\tag{2}\]
	here and below we assume all summation indicators like $k$ and $\ell$ are from $1$ to $n$, then
	\begin{align*}
	\ma{D}(I)&=\ma{D}(\sum_i \varepsilon_{ii})\\
			 &=\sum_i \ma{D}(\varepsilon_{ii})\\
			 &=\sum_{i,k,\ell} d_{ii}^{k\ell} \varepsilon_{k\ell}.
	\end{align*}
	Thus for any $1\leq s,t\leq n$,
	\[
	\begin{aligned}
	\ma{D}(I)\varepsilon_{st}&=\sum_{i,k,\ell} d_{ii}^{k\ell} \varepsilon_{k\ell} \varepsilon_{st}\\
					 &=\sum_{i,k,\ell} d_{ii}^{k\ell} \delta_{kt} \varepsilon_{s\ell}\\
					 &=\sum_{i,\ell} d_{ii}^{t\ell} \varepsilon_{s\ell},
	\end{aligned}\tag{3}
	\]
	and from $(2)$, 
	\[\ma{D}(\varepsilon_{st})=\sum_{k,\ell} d_{st}^{k\ell} \varepsilon_{k\ell}.\tag{4}\]
	Now by $(1)$ and notice that $\{\varepsilon_{ij}\}$ is a basis, we know that $(3)$ and $(4)$ are equal and 
	\[\big(d_{st}^{k\ell}=0\;\mbox{if}\;k\neq s\big)\quad\mbox{and}\quad d_{st}^{s\ell}=\sum_{i} d_{ii}^{t\ell}.\]
	From this we immediately get for any $k,s,t,\ell$
	\[d_{st}^{k\ell}=\delta_{ks} d_{tt}^{t\ell}.\tag{5}\]
	which is the equivalent condition we need. In other words, If we arbitrarily choose $n^2$-many numbers from $F$ and make them constructed all $d_{tt}^{t\ell}$-s and define other $d$-s by $(5)$. Then we use all $d$-s to define $\ma{D}$ by $(1)$ and surely such $\ma{D}$ exists and uniquely decided by $d$-s and is a linear mapping from $U$ to $U$, which completes our proof.
	\end{description}
\end{proof}

\section{Image and Kernel}

\begin{pro}%6
	Suppose $\ma{A},\ma{B},\ma{C}$ are all linear transformation on $(U,F)$, show that
	\[\rho(\ma{AB})+\rho(\ma{BC})\leq \rho(\ma{B})+\rho(\ma{ABC}).\]
\end{pro}
\begin{proof}
	Let $V=\IM\ma{B}<U$, then since $\{\ma{AB}x\colon x\in U\}=\{\ma{A}y\colon y\in V\}$, we know that
	\[\IM\ma{AB}=\IM \ma{A}|_{V},\]
	and thus $\rho(\ma{B})-\rho(\ma{AB})=\dim\Ker(\ma{A}|_V)$. Similarly we know $\rho(\ma{BC})-\rho(\ma{ABC})=\dim\Ker(\ma{A}|_W)$
	where $W=\IM\ma{BC}<V<U$. Now our proof can be completed by noticing that $\Ker(\ma{A}|_W)\subset \Ker(\ma{A}|_V)$.
\end{proof}

\begin{pro}%7
	Suppose that $f\colon U\to U$ is a linear mapping and $\rho(f)=1$. Show that there is some unique $\la\in F$ such that $f^2=\la f$. More over, when $\la\neq 1$, $I-f$ is a bijection where $I$ is the identical mapping on $U$.
\end{pro}
\begin{proof}
	Since $\rho(f^2)\leq \rho(f)=1$, we know $f\neq 0$ and $\rho(f^2)=0$ or $\rho(f^2)=1$. For the first case, $f^2=0$, then $\la=0$ is the unique solution of $\la$. As for the second one, notice that $f^2(U)=f(U)$, we know that if $f(\al_0)$ is the basis of it, then there is some $\la$ such that $f^2(\al_0)=\la f(\al_0)$. Then we assume that $\al_1,\al_2,\dots,\al_{n-1}$ is a basis of $\Ker f$, then we can extension it to a basis of $U$, i.e, $\al_0,\dots, \al_{n-1}$. Easy to know that $f(\al_0)$ now is indeed a basis of $f(U)$. Now for any $\al=\sum_i k_i \al_i \in U$, we know
	\[f^2(\al)=\sum_i k_i f^2(\al_i)=k_0 f^2(\al_0)=k_0\la f(\al_0)=\la f(\al).\]
	The uniques of $\la$ is obvious. Now suppose $\la \neq 1$, we have
	\[(\la-1)I=f^2-\la f+(\la-1)I=(I-f)\big((\la-1)I-f\big).\]
\end{proof}

\begin{pro}%10
	Suppose $A,B\in F^{m\times n}$, show that $\rank(A+B)=\rank(A)+\rank(B)$ if and only if there is some invertible matrices $P\in F^{m\times m}$ and $Q\in F^{n\times n}$ such that 
	\[PAQ=R=\begin{pmatrix} I_r & 0\\0 & 0\end{pmatrix},\quad PBQ=S=\begin{pmatrix} 0 & 0\\0 & I_s\end{pmatrix},\]
	where $r=\rank(A),s=\rank(B)$ and $r+s\leq \min(m,n)$.
\end{pro}
\begin{proof}
	Suppose $U,V$ are linear spaces with order $n$ and $m$ respectively, and we define $f,g\colon U\to V$ as a linear extension of 
	\begin{align*}
	f(\al_1',\dots,\al_n')&=(\beta_1',\dots,\beta_m')A,\\
	g(\al_1',\dots,\al_n')&=(\beta_1',\dots,\beta_m')B.\tag{1}
	\end{align*}
	where $(\al_j'),(\beta_j')$ are basis of $U,V$ resp. Since $f+g$ is also linear and $\rho(f+g)=\rank(A+B)=s+t$, there are some basis $\al_j$ and $\beta_j$ of $U,V$ such that
	\[(f+g)(\al_1,\dots,\al_n)=(\beta_1,\dots,\beta_m)(R+S).\]
	Easy to find out that $(\beta_1,\dots,\beta_m)R\in f(U)$ and $(\beta_1,\dots,\beta_m) S\in g(U)\;(*)$. 
	Now notice that
	\begin{align*}
		\rho(f+g)&\leq \dim f(U)+g(U)\\
				 &=\rho(f)+\rho(g)-\dim f(U)\cap g(U)\\
				 &\leq \rho(f)+\rho(g)\\
				 &=\rho(f+g),
	\end{align*}
	one can easily prove that $(f+g)U=f(U)\oplus g(U)$. Thus from $(*)$ we know 
	\begin{align*}
	f(\al_1,\dots,\al_n)&=(\beta_1,\dots,\beta_m)R,\\
	g(\al_1,\dots,\al_n)&=(\beta_1,\dots,\beta_m)S.\tag{2}
	\end{align*}
	From $(1)$ and $(2)$ and suppose that $(\al_j)=(\al_j')Q$ and $(\beta_j')=(\beta_j)P$, one can derive that
	\begin{align*}
	f(\al_1,\dots,\al_n)&=(\beta_1,\dots,\beta_m)PAQ,\\
	g(\al_1,\dots,\al_n)&=(\beta_1,\dots,\beta_m)PBQ.\tag{3}
	\end{align*}
	Compare to $(2)$, we get $PAQ=R$ and $PBQ=S$.
\end{proof}

\section{Linear Transformation}

\begin{pro}%6\
	Suppose $\ma{A}\colon V\to V$ is linear, prove that
	\begin{description}
	\item[(a)] There is some $0\neq f(x)\in F[x]$, such that $f(\ma{A})=0$,
	\item[(b)] There is some $0\neq f(x)\in F[x]$, such that $\deg f\leq n^2$ and $f(\ma{A})=0$.
	\end{description}
\end{pro}
\begin{proof}
	Surely the Cayley-Hamilton Theorem says that the order $n$ characteristic polynomial of $\ma{A}$ is already what we need here, but we give a simpler non-constructed proof by considering the set of all items we are interested in. Before that we should notice that $f(\ma{A})=0$ if and only if there is some matrix $A$ of $\ma{A}$ such that $f(A)=0$.
	\begin{description}
	\item[(a)] Define $\varphi\colon F[x]\to F[A]$ by $\varphi(f(x))=f(A)$, then 
	\[F[x]/\Ker\varphi\cong \IM\varphi <F[A]<F^{n\times n}.\]
	Notice that $\dim F[x]=+\infty$, thus $\dim \Ker \varphi=+\infty$.
	\item[(b)] Notice that $\{I,A,A^2,\dots, A^{n^2}\}$ is linear dependent, i.e, there is are not all zero $k$-s such that
	$k_0I+k_1 A+\dots k_{n^2}A^{n^2}=0$. Now define $f(x)=k_0+k_1x+\dots+k_{n^2}x^{n^2}\in F$, then $f(x)\neq 0$, $\deg f\leq n^2$ and $f(A)=0$.
	\end{description}
\end{proof}

\begin{pro}%9
	Supoose $f\colon V\to V$ is linear and $k$ is any given positive integer, then $f^k(V)=f^{2k}(V)$ if and only if $V=\Ker(f^k)\oplus f^k(V)$.
\end{pro}
\begin{proof}
	For the necessity, notice that $V/\Ker{f^k}\cong f^k(V)$, $V/\Ker{f^{2k}}\cong f^{2k}(V)$, thus $\Ker f^k=\Ker f^{2k}$. For any $f^k(x)\in \Ker{f^k}$, we know that $f^{2k}(x)=f^k(f^k(x))=0$, thus $x\in \Ker f^k$ and $f^k(x)=0$, which means $\Ker f^k+f^k(V)=\Ker f^k\oplus f^k(V)$. Given any $x\in V$, there is some $y\in V$ such that $f^k(x)=f^{2k}(y)$, thus we know $x=x-f^k(y)+f^k(y)\in \Ker f^k\oplus f^k(V)$ and get $V=\Ker f^k\oplus f^k(V)$. 

	As for the sufficiency, we only need to prove that $f^k(V)\subset f^{2k}(V)$. Given any $x\in V$, there is some $y\in V$ such that $x-f^k(y)\in\Ker f^k$. Thus $f^k(x)=f^{2k}(y)\in f^{2k}(V)$.

	In this case, we have $V=\Ker (f^k)\oplus f^k(V)$ and
	\begin{align*}
	\Ker f^k&=\Ker f^{2k}=\Ker f^{4k}=\cdots,\\
	f^k(V)&=f^{2k}(V)=f^{4k}(V)=\cdots.
	\end{align*}
	Moreover, notice that Given any $n\geq k$, there is some unique $q\geq 0$, such that $2^qk\leq n<2^{q+1} k$, thus
	$f^{2^{q+1} k}(V)\subset f^n(V)\subset f^{2^q k}(V)$ and get
	\begin{align*}
	\Ker f^k&=\Ker f^{k+1}=\Ker f^{k+2}=\cdots,\\
	f^k(V)&=f^{k+1}(V)=f^{k+2}(V)=\cdots.
	\end{align*}
	Thus we know that for any integer $1\leq k< j$, $f^k(V)=f^j(V)$ if and only if $f^k(V)=f^{k+1}(V)=\cdots$, if and only if $V=\Ker f^k\oplus \IM f^k$.
	\end{proof}

\begin{pro}%13
	Suppose $A\in\m{C}^{n\times n},\;f(x),g(x)\in\m{C}[x]$ and $\big(f(x),g(x)\big)=1$, then
	\[\rank f(A)+\rank g(A)=n+\rank f(A)g(A).\]
\end{pro}
\begin{proof}
	Let $A$ be the matrix of linear transformation $\ma{A}$ on space $V$. If $f(x)=\sum_i a_i x^i\in\m{C}[x]$, we define $f(\ma{A})=\sum_i a_i \ma{A}^i$ is also a linear transformation. Thus easy to know $f(\ma{A})g(\ma{A})(V)=f(\ma{A})\big(\IM g(\ma{A})\big)$, thus if we let $U=\IM g(\ma{A})<V$, then
	\begin{align*}
	\rank g(A)-\rank f(A)g(A)&=\dim\Ker f(\ma{A})|_U,\\
	\rank I-\rank f(A)&=\dim\Ker f(\ma{A}).
	\end{align*}
	Thus we only need to prove 
	\[\Ker f(\ma{A})\subset \Ker f(\ma{A})|_U.\tag{1}\]
	Given any $\al\in V$ such that $f(\ma{A})(\al)=0$, since that $(f,g)=1$, we can find $u(x),v(x)\in \m{C}[x]$ such that $uf+vg=1$. Thus
	\[\al=u(\ma{A})f(\ma{A})(\al)+v(\ma{A})g(\ma{A})(\al)=g(\ma{A})v(\ma{A})(\al)\in U,\]
	and $(1)$ is proved. 
\end{proof}

\section{Invariant Subspaces}

\begin{pro}%3
	Suppose $f\colon V\to V$ is linear and $0\neq \al\in V$, then 
	\[U=\Span{\al,f(\al),\dots,f^k(\al),\dots}\]
	 is invariant under $f$. Moreover, if $\dim U=r$, then $A=\{\al,f(\al),\dots,f^{r-1}(\al)\}$ is a basis of $U$.
\end{pro}
\begin{proof}
	$U$ is obviously an invariant subspace of $V$. In order to prove the second conclusion, we only need to prove that $A$ itself is linear independent or $f^r(\al)\in \Span{A}$. Suppose that $A$ is not independent, then we let $t$ be the minimum number that $\{\al,f(\al),\dots,f^t(\al)\}$ is dependent, since $\al\neq 0$, we know that $1\leq t\leq r-1$. Surely there is not all zero numbers $k$-s such that
	\[k_0+k_1\al+\dots+k_t f^t(\al)=0.\]
	If $k_t=0$, then easy to know that $k_0=\dots=k_{t-1}=0$ since $\{\al,\dots,f^{t-1}(\al)\}$ is independent, which leads to a contradiction. Thus we know $f^t(\al)\in \Span{\al,\dots,f^{t-1}(\al)}$. Thus
	\[f^r(\al)=f^{r-t}\big(f^t(\al)\big)\in \Span{f^{r-t}(\al),\dots,f^{r-1}(\al)}\subset \Span{A}.\]
	Thus we know for all $k>r$, we have 
	\[f^k(\al)\in f^{k-r}\big(\Span{A}\big)\subset \Span{A},\]
	which means that $U=\Span{A}$.
\end{proof}

\begin{pro}%8
	Suppose $\la_1,\dots,\la_n\in F$ are mutually different and $f\colon V\to V$ has the matrix $\diag(\la_1,\dots,\la_n)$ under the basis $\{\al_1,\dots,\al_n\}$ of $V$. Find out all invariant subspaces under $f$.
\end{pro}
\begin{proof}
	First we notice that $V=\Span{\al_1}\oplus\Span{\al_2}\oplus\dots\oplus\Span{\al_n}$, one can easily find out that the subspace $U=\Span{\al_{i_1}}\oplus\Span{\al_{i_2}}\oplus\dots\oplus\Span{\al_{i_r}}$ or $U=\{0\}$ is invariant under $f$ where $1\leq r\leq n$ and $1\leq i_1< \dots< i_r\leq n$. Now we prove that \textbf{any invariant space has the form above}\;$(*)$. 

	Notice that any non-trivial subspace $W$ has a basis in the form $A(\al_1,\dots,\al_n)^T$ where $A\in F^{n\times n}$ with $1\leq \rank(A)<n$. Easy to know that for any invertible matrix $P$, $PA(\al_1,\dots,\al_n)^T$ is also a basis of $W$. Now we let $P$ be the specific matrix that it can turn $A$ into the \textbf{reduced row echelon form}. Without losing generality, we can assume that
	\[PA=\begin{pmatrix}
	1 & & & a_{11} & \dots & a_{1r}\\
	  &\ddots &  &\vdots& &\vdots\\
	  & & 1 & a_{s1} & \dots & a_{sr}\\
	  &0&   &        & 0 &\end{pmatrix},\]
	where $1\leq s=\rank(A)\leq n-1$.
	Surely we can get this form by resorting columns of $PA$ and rows of $(\al_1,\dots,\al_n)^T$ and re-naming the first $s$-many rows of row-resorted $(\al_1,\dots,\al_n)^T$ as $\al_1,\dots,\al_s$. Then if we let
	\[
	\left\{
	\begin{array}{c}
	\begin{matrix}
	\al_1+a_{11}\al_{s+1}+\dots+a_{1r}\al_{s+r}=\beta_1\\
	\al_2+a_{21}\al_{s+1}+\dots+a_{2r}\al_{s+r}=\beta_2\\
	\dots\\
	\al_s+a_{s1}\al_{s+1}+\dots+a_{sr}\al_{s+r}=\beta_s,
	\end{matrix}
	\end{array}
	\right.	
	\]
	one can prove that $W=\Span{\beta_1,\dots,\beta_s}$. Since $W$ is invariant, we have $f(\beta_j)\in W$ for all $j$.
	Thus 
	\begin{align*}
	f(\beta_1)&=\la_1\al_1+a_{11}\la_{s+1}\al_{s+1}+\dots+a_{1r}\la_{s+r}\al_{s+r}\\
			  &=\mu_1\beta_1+\mu_2\beta_2+\dots+\mu_s\beta_s.\tag{1}
	\end{align*}
	Easy to know that $\mu_1=\la_1$ and $\mu_2=\dots=\mu_s=0$. Thus 
	\[f(\beta_1)=\la_1\al_1+a_{11}\la_1 \al_{s+1}+\dots+a_{1r}\la_1 \al_{s+r}.\tag{2}\]
	Comparing $(1)$ and $(2)$ we know that $a_{1t} \la_{s+t} \al_{s+t}=a_{1t}\la_1 \al_{s+t}$ for all $1\leq t\leq r$, which will lead to that $a_{1t}=0$ and we have shown that $(*)$ is true. Finally we know that all invariant subspaces form a set as
	\[\ma{U}=\{\Span{\al_{i_1}}\oplus\Span{\al_{i_2}}\oplus\dots\oplus\Span{\al_{i_r}}\colon 0\leq r\leq n,\;1\leq i_1<i_2
	<\dots <i_r\leq n\},\]
	and $|\ma{U}|=2^n$.
\end{proof}

\section{Eigenvalues and Eigenvectors}

\begin{pro}%3
	Suppose $\la_1,\la_2,\dots,\la_n$ are all the Eigvalues of order $n$ matrix $A$ and $f(x)\in \m{C}[x]$, then $f(A)$ have Eigvalues as $f(\la_1),f(\la_2),\dots,f(\la_n)$.
\end{pro}

\begin{pro}%7,8,9
	Suppose $A,B\in\m{C}^{n\times n}$, the characteristic polynomial (char-poly) is $\varphi(\la)$. Define $\ma{P}\colon \m{C}^{n\times n}\to\m{C}^{n\times n}$ as $\ma{P}(X)=AX-XB$. Then the following statements are equivalent.
	\begin{description}
	\item[(a)] There is no common Eigvalue between $A$ and $B$.
	\item[(b)] $\varphi(B)$ is invertible.
	\item[(c)] For $X\in\m{C}^{n\times n}$, the equation $AX=XB$ has only zero solution.
	\item[(d)] $\ma{P}$ is a bijection.
	\end{description}
\end{pro}
\begin{proof}
	Suppose $g$ is the char-poly of $B$ and $\varphi(\la)=(\la-\la_1)\cdots(\la-\la_n)=a_0+a_1\la+\dots+\la^n$, then 
	\begin{description}
	\item [(a) $\Rightarrow$ (b)] $\varphi(B)=(B-\la_1I)\cdots(B-\la_nI)$ and for all $1\leq j\leq n$, $|\la_jI-B|\neq 0$ since $A$ and $B$ share no common Eigvalues. 
	\item [(b) $\Rightarrow$ (c)] Suppose $X$ is any given solution, then $A^k X=X B^k$ for all $k\geq 0$. Thus $0=\varphi(A)X=X\varphi(B)$, which means $X=0$.
	\item [(c) $\Leftrightarrow$ (d)] Notice $\Ker \ma{P}=\{X\in\m{C}^{n\times n}\colon AX=XB\}=\{0\}$. Surely that (c) is equivalent to (d).
	\item [(c) $\Rightarrow$ (a)] Suppose $\la$ is a common Eigvalue of $A$ and $B$, then easy to drive that $\la$ is also an Eigvalue of $B^T$. There are some row vectors $\al,\beta\neq 0$ such that $A\al^T=\la\al^T,\;B^T\beta^T=\la B^T$, thus $A\al^T\beta=\la\al^T\beta=\al^T\beta B$. Thus $\al^T\beta\neq 0$ is a nonzero solution, which is a contradiction.
	\end{description}
\end{proof}

\begin{pro} 
	The min-poly of quasi diagonal matrix $\diag\{A_1,A_2,\dots,A_n\}$ is the least common multiple of min-polys of $A$-s.
\end{pro}
\begin{proof}
	Given any poly $g$, $g\diag\{A_1,A_2,\dots,A_n\}=\diag\{g(A_1),g(A_2),\dots,g(A_n)\}$, thus $d_j\mid d$ for all $j$ and $d_{\mbox{c}}:=[d_1,d_2,\dots,d_n]\mid d$. Also, 
	\[d_{\mbox{c}}\diag\{A_1,A_2,\dots,A_n\}=\diag\{d_{\mbox{c}}(A_1),d_{\mbox{c}}(A_2),\dots,d_{\mbox{c}}(A_n)\}=0,\] thus $d\mid d_{\mbox{c}}$ and $d=[d_1,d_2,\dots,d_n]$.
\end{proof} 

\begin{pro}%20
	Suppose the order $n$ square matrix $A$ is 
	\[A=\begin{pmatrix}
	a_1(1-a_1) & -a_1a_2 & \cdots & -a_1a_n\\
	-a_2a_1 & a_2(1-a_2) & \cdots & -a_2a_n\\
	\vdots  & \vdots     & \ddots & \vdots \\
	-a_na_1 & -a_na_2    & \cdots & a_n(1-a_n)\\\end{pmatrix}.\]
	In what condition (if and only if) $A$ will be invertible and find $A^{-1}$ when $A$ is invertible.
\end{pro}
\begin{proof}
	Notice we can rewrite $A$ as $A=P(I_n-B)$ where
	\[P=\diag\{a_1,a_2,\dots,a_n\},\quad B=
	\begin{pmatrix}
	a_1 & a_2 & \cdots & a_n\\
	a_1 & a_2 & \cdots & a_n\\
	\vdots & \vdots & & \vdots\\
	a_1 & a_2 & \cdots & a_n\end{pmatrix}.\]
	Thus $\det A\neq 0$ if and only if $a_1,a_2,\dots,a_n\neq 0$ and $\det (I_n-B)\neq 0$. Now let 
	\[C=(1\; 1\cdots 1)^T,\quad D=(1\;1 \cdots 1)P,\]
	easy to know $B=CD$. By Problem $3.3.6$ (cf. Textbook Page 113), $I_n-CD$ is invertible if and only if $1-DC$ is invertible, i.e., $s:=a_1+a_2+\dots+a_n\neq 1$. Also when $I_n-CD$ is invertible, 
	\begin{align*}
	(I_n-CD)^{-1}&=I_n+C(1-DC)^{-1}D\\
				 &=I_n+C(1-s)^{-1}D\\
				 &=I_n+\dfrac{B}{1-s}.
	\end{align*}
	Another way to find $(I_n-B)^{-1}$: since $B^2=sB$, we know $(I_n-B)\big((s-1)I_n-B\big)=s-1$. Finally we know $A$ is invertible if and only if $a_j\neq 0$ for $1\leq j\leq n$ and $s\neq 1$, also when $A$ is invertible, 
	\begin{align*}
	A^{-1}&=(I_n-B)^{-1}P^{-1}\\
		  &=\Big(I_n+\frac{B}{1-s}\Big)P^{-1}\\
		  &=\frac{J_n}{1-s}+\diag\{a_1^{-1},a_2^{-1},\dots,a_n^{-1}\},	 	
	\end{align*}
	where $J_n=(1)_{n\times n}$ is the matrix in which all elements are $1$.
\end{proof}
\section{Eigenspaces}

\begin{pro}%8
	$V$ is some $n$-dimensional ($n$ is finite for sure) vector space over $\m{C}$ with dimension $n$ and $P$ is some non-empty class of linear transforms on $V$ itself. If any two of transforms in $P$ can exchange to each other, then all transforms in $P$ share some common eigenvector.
\end{pro}
\begin{proof}
	First we prove that if $f,g$ are exchangeable, then every eigenspace of $f$ is invariant for $g$. Suppose $\la$ is some eigenvalue of $f$, then $V_{\la}$ is a non-zero space over $\m{C}$, for any $x\in V_{\la}$, $fg(x)=gf(x)=\la g(x)$, thus $g(x)\in V_{\la}$, which means $V_{\la}$ is invariant for $g$. Now district $g$  on $V_{\la}$, since $V_{\la}\neq \{0\}$, we know $g|V_{\la}$ must have eigenvector(s) in $V_{\la}$, some of them construct an eigenspace for $g|V_{\la}$ (and thus all non-zero vectors in them are(is) eigenvector(s) with the same eigenvalue for $g$), say $U$, then $\{0\}\neq U\subset V_{\la}$. Since $V$ is space over $\m{C}$, thus all vectors in $U$ are common eigenvectors for $f$ and $g$. Assume $S\subset P$ has the properties that $S$ is linear independent and for any $y\in P\ba S$, $S\cup\{y\}$ is dependent, then there are at most $n$-many vectors in $S$ and $P\subset\Span{S}$. Thus we only need to prove that all vectors in $S$ share some common eigenvector, which is easy to derive by induction.
\end{proof}