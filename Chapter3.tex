\chapter{Matrices}
\section{Algebraic Operations to Matrices}
\begin{pro}%5
	Suppose $A$ is an order $n$ diagonal matrix with mutually different diagonal elements. Show that an order $n$ square matrix $B$ is commutative to $A$ if and only if $B$ is diagonal. Also consider the generalized situation:
	\[A=\diag(\la_1 I(k_1),\la_2 I(k_2),\ldots,\la_t I(k_t)),\]
	where $\seq{\la}{1}{t}$ are mutually different and $k_1+\cdots+k_t=n$.
\end{pro}
\begin{proof}
	Assume $AB=C$ and $BA=D$, then $c_{ij}=\sum_k a_{ik}b_{kj}=a_{ii}b{ij}$ and $d_{ij}=\sum_k b_{ik}a{kj}=a_{jj}b_{ij}$, thus for $i\neq j$, $c_{ij}=d_{ij}$ if and only if $b_{ij}=0$, i.e. $B$ is a diagonal matrix. As for the generalized situation, similarly we know $B$ is commutative to $A$ if and only if $B$ is a quasi-diagonal matrix in the form
	\[B=\diag(B_1,B_2,\ldots,B_t)\]
	where $B_i$ is any square matrix in $k_i$ order, $(1\leq i\leq t)$.
\end{proof}

\begin{pro}%6
	Prove that (a) The multiplication of two upper triangular square matrix is also upper triangular. (b) If $\seq{A}{1}{n}$ are $n$ upper triangular matrix in $n$ order, and the $(i,i)$ element of $A_i\;(1\leq i\leq n)$ is zero, then $A_1A_2\cdots A_n=0$.
\end{pro}
\begin{proof}
	\begin{description}
		\item[(a)] Assume $AB=C$ where $A$ and $B$ are both upper triangular, then for all $1\leq j<i\leq n$, we have 
		\[c_{ij}=\sum_k a_{ik}b_{kj}=\sum_{i\leq k\leq j} a_{ik}b_{kj}=0,\]
		thus $C$ is also upper triangular. \\
		\item[(b)] Assume $B=A_1\cdots A_n$ and for $1\leq m\leq n$, $A_m=(a_{ij}^m)$, then for $1\leq i<j\leq n$,
		\[b_{ij}=\sum_H a_{ik_1}^1 a_{k_1k_2}^2\cdots a_{k_{n-1}k_n}^n\]
		where $H=\{(\seq{k}{1}{n}:i\leq k_1\leq k_2\leq\cdot\leq k_n=j\}$. Now we prove that for any $K:=\seq{k}{1}{n}\in H$, 
		\[c(K):=a_{ik_1}^1 a_{k_1k_2}^2\cdots a_{k_{n-1}k_n}^n=0.\]
		Consider $k_{i}$, if $k_i=i$, then $k_1=\ldots=k_{i-1}=k_i=i$, thus $a_{k_{i-1},k_{i}}^k=a{ii}^i=0$ and then $c(K)=0$. For $k_i>i$, if we want $c(K)$ also to be zero, we only need to prove that for any $1\leq i<j<n$, if
		\[i<k_i\leq k_{i+1}\leq \cdots\leq k_j\leq j,\tag{1}\]
		then there is some $\ell$, such that $i+1\leq \ell\leq j$ and $k_{\ell}=k_{\ell-1}=l$.
		Notice that if $j=i+1$ then if $(1)$ is right, i.e. $i<k_i<k_{i+1}=i+1$, then $k_i=k_{i+1}=i+1$, thus the bases is already true. Now assume that all $(i,j)$ such that $\Delta=j-i$, the assumption above is right, we prove that it is also true for $(i-1,j)$.

		If $k_{i-1}>i$, then $k_i>i$, this is the assumption for $(i,j)$ and thus is true. If $k_{i-1}=i$, then $k_i\geq i$. Assume that $k_i=i$, then $k_{i-1}=k_i=i$ and then $a_{k_{i-1}k_i}^i=a_{ii}^i=0$ and $c(K)=0$. Thus we finally know that for any $K=\in H$, we have $c(K)=0$, thus $b_{ij}=0$ for all $1\leq i<j<n$. It's easy to show that for all $1\leq i\leq n$, we have $b_{ii}=0$ and according to (a), $b_{ij}=0$ for all $i<j$. Thus we know $b_{ij}=0$ for all $1\leq i,j\leq n$, i.e. $A_1A_2\cdots A_n=0$.
	\end{description}
\end{proof}

\begin{proof}
	We give another proof by conduction of (b). Suppose $A$ and $B$ are two upper triangular matrix in order $n$, where the column $1$ to column $k$ of $A$ are all zero column and the $(k+1,k+1)$ element of $B$ is zero for some integer $1\leq k<n$, then the column $1$ to column $k+1$ of $AB$ are all zero column. Now just notice that the first column of $A_1$ is zero column and the $(2,2)$ element of $A_2$ is zero.
\end{proof}

\begin{pro}%7
	Suppose a square matrix $A$ is nilpotent if there is some positive integer $k$, such that $A^k=0$ and we call the smallest one of such $k$s is the nilpotent index of $A$. Show that if $A$ is an upper triangular matrix, then (a) $A$ is nilpotent if and only if the diagonal elements are all zero. (b) If $A$ is nilpotent, then the nilpotent index is less or equal to its order.
\end{pro}

\begin{proof}
	Write $B=A^k$, then easy to know $b_{ii}=a_{ii}^k$, thus if $B=0$, we know $a_{ii}=0$. By problem $3.1.6$, we already know that if $a_{ii}=0$ for all $i$, then $A^n=0$. Thus we proved (a) and (b).
\end{proof}

\begin{pro}%8
	Fina all order 2 nilpotent square.
\end{pro}
\begin{proof}
	Write
	\[A=\begin{pmatrix}a&b\\c&d\end{pmatrix}\]
	Then if $b=0$ or $c=0$, then $A$ is nilpotent if and only if $a=d=0$. Now we assume that $b,c\neq 0$, then easy to know
	\[A^n=\begin{pmatrix}aa^n+bc^n&ab^n+bd^n\\ca^n+dc^n&cb^n+dd^n\end{pmatrix}\]
	Easy to know $a\neq 0$. Thus we write $c=ka$ where $k\neq 0$, then easy to know that $b=-a/k^n$ and $d=-a/k^{n-1}$. Finally we know $A$ is nilpotent if and only if $A=0$ or
	\[A=\begin{pmatrix}0&\la\\0&0\end{pmatrix}\;\mbox{or}\;\begin{pmatrix}0&0\\\la&0\end{pmatrix},\]
	where $\la \neq 0$ or
	\[A=\begin{pmatrix} a&-a/k^n\\ka&-a/k^{n-1}\end{pmatrix}\]
	where $k,a\neq 0$ and $n$ is positive integer.
\end{proof}

\begin{pro}%9
	If $A^2=I$, then we call $A$ is an involutory matrix. Try to find all such matrix in order $2$.
\end{pro}
\begin{proof}
	Easy to know 
	\[A=\pm\begin{pmatrix}1&0\\0&1\end{pmatrix}\;\mbox{or}\;\begin{pmatrix}\la&\la_2\\
	\la_1&-\la\end{pmatrix}\]
	where $\la\neq 0$ and $\la^2+\la_1\la_2=1$.
\end{proof}

\begin{pro}%10
	Try to find all $A$ in order two such that $A^2=A$.
\end{pro}
\begin{proof}
	From Problem $3.1.11$ we know $A=(B+I)/2$ where $B$ is an involutory matrix.
\end{proof}

\begin{pro}%11
	Assume that $A,B$ are both order $n$. Show that if $A^2=I$, then $B=(A+I)/2$ has the property that $B^2=B$. If $B^2=B$, then $A=2B-I$ has the property that $A^2=I$.
\end{pro}
\begin{proof}
	Routine.
\end{proof}

\begin{pro}%12
	We call $A$ is symmetric if $A=A^T$. Show that
	\begin{description}
		\item[(a)] If $B\in F^{m\times n}$, then $BB^T$ is symmetric.
		\item[(b)] If $A$ and $B$ are symmetric order $n$ matrix, then $AB$ is symmetric if and only if $AB=BA$.
	\end{description}
\end{pro}
\begin{proof}
	We only need to show that if $A\in F^{m\times n}$ and $B\in F^{n\times p}$, then $(AB)^T=B^T A^T$.
	Assume $C=AB$, and denote $A^T=(a_{ij}^T)$ for all matrix, then
	\[c_{ij}^T=c_{ji}=\sum_k a_{jk}b_{ki}=\sum b_{ik}^Ta_{kj}^T,\]
	thus $C^T=B^T A^T$.
\end{proof}

\begin{pro}%13
	We call $A$ is antisymmetric if $A^T=-A$. Show that if $A$ and $B$ are both antisymmetric, then
	\begin{description}
		\item[(a)] $AB$ is antisymmetric if and only if $AB=-BA$.
		\item[(b)] $AB$ is symmetric if and only if $AB=BA$.
	\end{description}
\end{pro}
\begin{proof}
	Routine.
\end{proof}

\begin{pro}%14
	SHow that any square matrix can be uniquely dis-composited to be the sum of a symmetric square matrix $S$ and a antisymmetric one $K$.
\end{pro}
\begin{proof}
	Let $S=(A+A^T)/2$ and $K=(A-A^T)/2$, easy to know that $A=S+K$ and $S$ is symmetric, $K$ is antisymmetric. Suppose $A=S+K$ where $S$ is symmetric and $K$ is antisymmetric, then $A^T=S^T+K^T=S-K$, thus $S=(A+A^T)/2$ and $K=(A-A^T)/2$.
\end{proof}

\begin{pro}%15
	We call a complex square matrix $A$ is Hermite if $\bar{A}=A^T$. Show that for all $A\in\m{C}^{m\times n}$, $A\bar{A}^T$ is Hermite.
\end{pro}
\begin{proof}
	Notice that $(A\bar{A}^T)^T=\bar{A}A^T=\overline{A\bar{A}^T}$.
\end{proof}

\begin{pro}%16
	Suppose that $A$ is a quasi-upper triangular square matrix whose diagonal blocks are all square matrix, then $A$ is nilpotent if and only if all diagonal blocks of $A$ are nilpotent.
\end{pro}
\begin{proof}
	Write $A\in F^{n\times n}$ in form
	\[A=\begin{pmatrix}A_1& &*\\ &\ddots& \\0& &A_t\end{pmatrix},\]
	where $A_i\in F^{k_i\times k_i}(1\leq i\leq t)$ and $k_1+\cdots+k_t=n$. Then easy to know for all positive integer $m$,
	\[A^m=\begin{pmatrix}A_1^m& &*\\ &\ddots& \\0& &A_t^m\end{pmatrix}.\]
	Thus if $A$ is nilpotent, then $A_i$s are all nilpotent. Conversely, assume that all $A_i$s are nilpotent, with nilpotent index $s_i$ respectively. Let $s=\max\{s_1,\ldots,s_t\}$, then 
	\[A^s=\begin{pmatrix}0& &*\\ &\ddots& \\0& &0\end{pmatrix}.\]
	Then similarly to the proof of Problem $3.1.6$ we know $A^{st}=0$, i.e. $A$ is also nilpotent.
\end{proof}

\begin{pro}%18
	Order $n$ matrix $A$ has exactly two one in every row and all other elements are zero, try to find all $A$s such that $A^2+2A=2J$, where $J$ is denoted by an order $n$ matrix whose all elements are one.
\end{pro}

\section{Binet-Cauchy Formula}
\begin{pro}%1
	Suppose $A\in F^{2\times 2}$. Show that $A^2-\tr(A)A+(\det A)I=0$.
\end{pro}
\begin{proof}
	Routine.
\end{proof}

\begin{pro}%2
	Show that there is no $A$ and $B$ in $F^{n\times n}$, such that $AB-BA=I$.
\end{pro}
\begin{proof}
	Notice that $\tr(AB-BA)=0$ but $\tr(I)=n$.
\end{proof}

\begin{pro}%3
	Do some proofs.
	\begin{description}
		\item[(a)] Prove Theorem $3.2.1$ by the fact that
		\[\det\begin{pmatrix}A&0\\-I&B\end{pmatrix}=\det A \det B,\]
		where $A,B\in F^{n\times n}$.
		\item[(b)] Prove Theorem $3.2.2$, i.e. the Binet-Cauchy formula.
	\end{description}
\end{pro}
\begin{proof}
	\begin{description}
	\item[(a)] Easy to know if we add $A(-I,B)$ to the first block row $(A,0)$, we will get
	\[C=\begin{pmatrix}0&AB\\-I&B\end{pmatrix}=\det A \det B.\]
	By Laplace, we know $\det C=\det (AB)$. Thus Theorem $3.2.1$ has been proved.
	\item[(b)] Suppose $\seq{\alpha}{1}{n}$ are the column vectors of $A$, then the product could be rewrote in the following form:
	\begin{eqnarray*}
		AB &=& (\seq{\alpha}{1}{n})\mat{b}{n}{s}\\
		&=&(b_{11}\alpha_1+\cdots+b_{n1}\alpha_n,\cdots,b_{1s}\alpha_1+\cdots+b_{ns}\alpha_n).
	\end{eqnarray*} 
	Now, we can \emph{expand} it based on the properties of determinant(here $s\leq n$ and $\seq{i}{1}{s}$ are mutually different)
	\begin{eqnarray*}
		|AB| &=& \sum_{v}
		\left[
			\sum_{i}
			\left|
				b_{v_1i_1}\alpha_{v1},b_{v_2i_2}\alpha_{v2},\ldots,b_{v_si_s}\alpha_{vs}
			\right|
			(-1)^{\tau(\seq{i}{1}{s})}
		\right]\\
		&&\vspace{20pt}\\
		&=& \sum_{v} |\alpha_{v_1},\ldots,\alpha_{v_s}|
		\left[
			\sum_{i}
			(-1)^{\tau(\seq{i}{1}{s})}
			b_{v_1i_1},\ldots,b_{v_si_s}
		\right]\\
		&&\vspace{20pt}\\
		&=& \sum_{v}
			A
			\begin{pmatrix}
				1,2,\cdots,s\\
				v_1,v_2,\cdots,v_s
			\end{pmatrix}
			\cdot B
			\begin{pmatrix}
				v_1,v_2,\cdots,v_s\\
				1,2,\cdots,s
			\end{pmatrix},
	\end{eqnarray*}
	where $v$ represents $1\leq v_1<\dots<v_s\leq n$ and $i$ represents $1\leq i_1<\cdots i_s\leq n$. When $s>n$, easy to know $|AB|$ will be expanded to $0$ since at least \emph{one} column vector of $A$ is choosed from \emph{two} columns of $AB$, and thus finish our proof.
	\end{description}
\end{proof}

\begin{pro}%4
	Do some computations.
	\begin{description}
		\item[(a)]
		\[\begin{vmatrix}s_0&s_1&s_2&\cdots&s_{n-1}&1\\s_1&s_2&s_3&\cdots&s_n&x\\
		& & &\cdots& &\\s_{n-1}&s_n&s_{n+1}&\cdots&s_{2n-2}&x^{n-1}\\
		s_n&s_{n+1}&s_{n+2}&\cdots&s_{2n-1}&x^n\end{vmatrix},\]
		where $s_k=x^k+x_2^k+\cdots+x_n^k,\;k=1\,2,\cdots$.
		\item[(b)]
		Suppose that $f_i(x)=a_{0i}+a_{1i}x+\cdots+a_{n-1,i}x^{n-1}$, and
		$D=\det(a_{ij})$ for all $0\leq i,j\leq n-1$. Find out
		\[\begin{vmatrix}f_0(x_1)&f_0(x_2)&\cdots&f_0(x_n)\\
						   f_1(x_1)&f_1(x_2)&\cdots&f_1(x_n)\\
						   & &\cdots& \\
						   f_{n-1}(x_1)&f_{n-1}(x_2)&\cdots&f_{n-1}(x_n)
		\end{vmatrix},\]
		\item[(c)]
		\[\begin{vmatrix}1^2&2^2&3^2&\cdots&n^2\\n^2&1^2&2^2&\cdots&(n-1)^2\\
		 & & &\cdots&\\2^2&3^2&4^2&\cdots&1^2\end{vmatrix},\]
		 \item[(d)]
		 \[\begin{vmatrix}a_1&a_2&a_3&\cdots&a_n\\
		 -a_n&a_1&a_2&\cdots&a_{n-1}\\-a_{n-1}&-a_n&a_1&\cdots&a_{n-2}\\
		 & & &\cdots&\\-a_2&-a_3&-a_4&\cdots&a_1\end{vmatrix}.\]
	\end{description}
\end{pro}
\begin{proof}
	\begin{description}
	\item[(a)]
	We let 
	\[A=\begin{pmatrix}1&\cdots&1&1\\x_1&\cdots&x_n& x\\ &\cdots& & \\x_1^n&\cdots&x_n^n&x^n
	\end{pmatrix},\]
	and let
	\[B=\begin{pmatrix}1&x_1&\cdots&x_1^{n-1}&0\\
	& & \cdots& & \\
	1&x_n&\cdots&x_n^{n-1}&0\\
	0&0&\cdots&0&1\end{pmatrix}.\]
	Then easy to know that the determinant is \[\det(AB)=\det(A)\det(B)=\prod_{i=1}^n(x-x_i)\prod_{1\leq i<j\leq n}(x_j-x_i).\]
	\item[(b)]
	We let $\Delta$ to be the matrix of $D$, and $A=(a_{ij})\in F^{n\times n}$ where $a_{ij}=x_i^{j-1}$, then the determinant is 
	\[\det(\Delta A)=D\det A=D \prod_{1\leq i<j\leq n}(x_j-x_i).\]
	\item[(c)]
	Let $f(x)=1+2^2x+3^2x^2+\cdots+n^2x^{n-1}$, then by Example $3.2.1$ (cf. page 102), we know the determinant is $\prod_{i=0}^{n-1}f(\omega^i)$, where $\omega=\exp(2\pi i/n)$.
	\item[(d)]
	We let $\omega_0,\omega_1,\ldots,\omega_{n-1}$ be all $n$ roots of $-1$, i.e., let
	\[\omega_k=\exp\big((2k+1)\pi i/n\big),\quad 0\leq k\leq n-1\]
	and let 
	\[A=\begin{pmatrix}1 &1&\cdots&1\\\omega_0&\omega_1&\cdots&\omega_{n-1}\\
	\omega_0^2&\omega_1^2&\cdots&\omega_{n-1}^2\\& & \cdots&\\
	\omega_0^{n-1}&\omega_1^{n-1}&\cdots&\omega_{n-1}^{n-1}\end{pmatrix}.\]
	Also let $E$ be the matrix of the determinant we want to calculate, then
	\[EA=A\diag(f(\omega_0),f(\omega_1),\cdots,f(\omega_{n-1}),\]
	where $f(x)=a_1+a_2x+\cdots+a_n x^{n-1}$.
	Thus we know $\det E=\prod_{k=0}^{n-1} f(\omega_k)$.
	\end{description}
\end{proof}
\begin{pro}%5
	Do some computation.
\end{pro}

\begin{pro}%6
	Omit
\end{pro}

\begin{pro}%7
	We call an order $n$ square real matrix $A$ orthogonal if $AA^T=I=A^TA$, show that
	\begin{description}
	\item[(a)] the determinant of an orthogonal matrix is $\pm 1$,
	\item[(b)] for any $1\leq k\leq n$ and $1\leq i_1<i_2<\cdots<i_k\leq n$,
	\[\sum_{J} A\begin{pmatrix}i_1&i_2&\cdots&i_k\\j_1&j_2&\cdots&j_k\end{pmatrix}^2=1.\]
	\end{description}
\end{pro}
\begin{proof}
	The (a) is obvious, as for the (b), notice that 
	\[A\begin{pmatrix}i_1&i_2&\cdots&i_k\\j_1&j_2&\cdots&j_k\end{pmatrix}=
	A^T\begin{pmatrix}j_1&j_2&\cdots&j_k\\i_1&i_2&\cdots&i_k\end{pmatrix}.\]
\end{proof}

\section{Inverses of Matrices}
\begin{pro}%1
	Find the inverse of following matrices.
	\begin{description}
		\item[(a)]
		\[A=\begin{pmatrix}1&1&1&1\\1&1&-1&-1\\1&-1&1&-1\\1&-1&-1&1\end{pmatrix},\]
		\item[(b)]
		\[B=\begin{pmatrix}1& &1\\ &\ddots& \\0& &1\end{pmatrix},\]
		\item[(c)]
		\[C=\begin{pmatrix}1&1&1&\cdots&1\\1&\omega&\omega^2&\cdots&\omega^{n-1}\\
		1&\omega^2&\omega^4&\cdots&\omega^{2(n-1)}\\& & &\cdots&\\
		1&\omega^{n-1}&\omega^{2(n-1)}&\cdots&\omega^{(n-1)^2}\end{pmatrix},\]
		where $\omega=\exp(2\pi/n)$,
		\item
		\[D=\begin{pmatrix}2&-1& & &0 \\-1&2&-1& &\\ &-1&\ddots&-1& \\
		& &-1&2&-1\\0& & &-1&2\end{pmatrix}.\]
	\end{description}
\end{pro}
\begin{proof}
	\begin{description}
	\item[(a)]
	Easy to know that $A^2=4I$, thus $A^{-1}=A/4$.
	\item[(b)] 
	Easy to know
	\[B^{-1}=\begin{pmatrix}1&-1& &0\\ &1&-1& \\ & &\ddots&-1\\
	0& & &1\end{pmatrix}.\]
	\item[(c)] 
	Notice the fact that for $0\leq j,k\leq n-1$, we have
	\begin{align*}
	&\begin{pmatrix}1&\omega^j&\cdots&\omega^{j(n-1)}\end{pmatrix}
	\begin{pmatrix}1\\\omega^{-k}\\\vdots\\\omega^{-k(n-1)}\end{pmatrix}\\
	&= 1+\omega^{j-k}+\omega^{2(j-k)}+\cdots+\omega^{(n-1)(j-k)}\\
	&=n\delta_{jk}.
	\end{align*}
	Thus if we define
	\[E=\begin{pmatrix}1&1&1&\cdots&1\\1&\omega^{-1}&\omega^{-2}&\cdots&\omega^{-(n-1)}\\
		1&\omega^{-2}&\omega^{-4}&\cdots&\omega^{-2(n-1)}\\& & &\cdots&\\
		1&\omega^{-(n-1)}&\omega^{-2(n-1)}&\cdots&\omega^{-(n-1)^2}\end{pmatrix},\]
	we know that $AE=EA=nI$, thus finally $C^{-1}=E/n$.
	\end{description}
\end{proof}

\begin{pro}%2
	Suppose that $A$ is an order $n$ nilpotent square matrix whose nilpotent index is $k$ and that $I-A$ is reversible, then $(I-A)^{-1}=I+A+\cdots+A^{k-1}$.
\end{pro}
\begin{proof}
	Define $f(x)=1+x+x^2+\cdots$, then $f(x)(1-x)=(1-x)f(x)=1$. Thus we know $f(A)(I-A)=(I-A)f(A)=I$. Fortunately, we have $f(A)=I+A+\cdots+A^{k-1}$.
\end{proof}

\begin{pro}%3
	If order $n$ square matrix $A$ is not reversible, then there is some order $n$ square matrix $B\neq 0$, such that $AB=BA=0$.
\end{pro}
\begin{proof}
	Easy to know that for any order $n$ square matrix $A$, we have
	\[A A^*=A^* A=\det A I.\]
	Since $A$ is not reversible, i.e., $\det A=0$, we know $A A^*=A^* A=0$.
\end{proof}

\begin{pro}%4
	Suppose $A$ is an upper triangular square matrix. Show that $A$ is reversible if and only if all its diagonal elements are not zero. Further more, we know that the inverse of $A$ is also upper triangular.
\end{pro}
\begin{proof}
	Easy to know $\det A=a_{11} a_{22}\cdots a_{nn}$, thus we proved the first conclusion. Actually, for any upper triangular square matrix $B$, we know that $B^*$ is also upper triangular, thus we proved the second conclusion.
\end{proof}

\begin{pro}%9
	Suppose $A^*$ is the adjoint order $n$ matrix $A$, show that $(\la A)^*=\la^{n-1} A^*$, where $\la\in F$.
\end{pro}
\begin{proof}
	First we show that $(AB)^*=B^*A^*$. Let $C=AB$, then easy to know
	\begin{align*}
	c_{ij}^*&=C_{ji}\\
			&=\sum_{k=1}^n A_{jk}B_{ki}\\
			&=\sum_{k} b_{ik}^* a_{kj}^*.
	\end{align*}
	Thus we know $C^*=B^*A^*$. Now Just notice that $(\la I)^*=\la^{n-1} I$ and then
	\[(\la A)^*=(\la I A)^*=A^*(\la I)^*=A^* \la^{n-1} I=\la^{n-1} A^*.\]
\end{proof}

\begin{pro}%10
	Suppose $A_{ij}$ is the algebraic cofactor of $a_{ij}$ in $A=(a_ij)$. Show that
	\begin{align*}
	\begin{vmatrix}A_{ik}&A_{jk}\\A_{i\ell}&A_{j\ell}\end{vmatrix}&=
	(-1)^{i+j+k+\ell}\\
	&A\begin{pmatrix} 
	1&2&\cdots&i-1&i+1&\cdots&j-1&j+1&\cdots&n\\
	1&2&\cdots&k-1&k+1&\cdots&\ell-1&\ell+1&\cdots&n\end{pmatrix},
	\end{align*}
	where $1\leq i<j\leq n,\; 1\leq k<\ell\leq n$.
\end{pro}
\begin{proof}
Add proof here later.
\end{proof}

\begin{pro}%11
	Suppose $A\in F^{m\times n},B\in F^{n\times m}$, $\la$ is an in-determinant, show that:
	\[\la^{n}\det(\la I-AB)=\la^{m}\det(\la I-BA).\]
\end{pro}
\begin{proof}
	Since $\la$ is an in-determinant, we can say that $\la\neq 0$, thus we only need to prove
	\[\frac{1}{\la^m} \det(\la I-AB)=\frac{1}{\la^n} \det(\la I-BA),\]
	i.e, we need to prove
	\[\det\bigg(I-\frac{1}{\sqrt{\la}} A \frac{1}{\sqrt{\la}}B\bigg)=\det\bigg(I-\frac{1}{\sqrt{\la}}B \frac{1}{\sqrt{\la}}A\bigg),\]
	which has been proved by Example $3.3.3$.
\end{proof}

\begin{pro}%12
	Suppose that $A,B\in\m{C}^{n\times n}$, and $i^2=-1$, show that
	\[\det\begin{pmatrix} A&-B\\B&A\end{pmatrix}=\det(A+iB)\det(A-iB).\]
\end{pro}
\begin{proof}
	Notice that
	\begin{align*}
		\begin{pmatrix}I & iI \\0 & I\end{pmatrix}
		\begin{pmatrix}A & -B \\B & A\end{pmatrix}&=
		\begin{pmatrix}A+iB & i(A+iB) \\B & A\end{pmatrix}\\&=
		\begin{pmatrix}A+iB & 0 \\0 & I\end{pmatrix}
		\begin{pmatrix}I & iI \\B & A\end{pmatrix}
	\end{align*}
	Thus we know that
	\[\det \begin{pmatrix}A & -B \\B & A\end{pmatrix}=\det(A+iB) 
	\det \begin{pmatrix}I & iI \\B & A\end{pmatrix}.\tag{1}\]
	Now notice that
	\[\begin{pmatrix}I & iI \\B & A\end{pmatrix}
	\begin{pmatrix}I & -iI \\0 & I\end{pmatrix}=
	\begin{pmatrix}I & 0 \\B & A-iB\end{pmatrix},\]
	thus
	\[\det \begin{pmatrix}I & iI \\B & A\end{pmatrix}=\det(A-iB).\tag{2}\]
	By $(1)$ and $(2)$, we proved the original conclusion.
\end{proof}

\begin{pro}%13
	Find the determinant of $A=(a_{ij})$, where $a_{ii}=x+a_i$ for $i=1,2,\ldots,n$ and $a_{ij}=a_j$ for all $i\neq j$.
\end{pro}
\begin{proof}
	Notice that
	\[A=xI+\begin{pmatrix}1\\\vdots\\1\end{pmatrix}
	\begin{pmatrix}a_1&\cdots& a_n\end{pmatrix},\]
	and easy to know that
	\[x \det\bigg(I_n+\begin{pmatrix}1\\\vdots\\1\end{pmatrix}
	\begin{pmatrix}a_1&\cdots&a_n\end{pmatrix}\bigg)
	=x^n \det\bigg(x
	+\begin{pmatrix}a_1&\cdots&a_n\end{pmatrix}
	\begin{pmatrix}1\\\vdots\\1\end{pmatrix}\bigg)\]
	by Problem $3.3.11$, now we know
	\[\det A=x^n+(a_1+\cdots+a_n)x^{n-1}.\]
\end{proof}

\begin{pro}%14
	Suppose $A\in\m{R}^{2n\times 2n}$, and
	\[A\begin{pmatrix}0 & I_n\\-I_n & 0\end{pmatrix}A^T=
	\begin{pmatrix}0 & I_n\\-I_n & 0\end{pmatrix}.\]
	Show that $\det A=1$.
\end{pro}
\begin{proof}
	We let
	\[A=\begin{pmatrix}A_1 & A_2\\ A_3 & A_4\end{pmatrix},\]
	then
	\[\begin{pmatrix}A_1 & A_2\\ A_3 & A_4\end{pmatrix}
	\begin{pmatrix}0 & I\\ -I & 0\end{pmatrix}
	\begin{pmatrix}A_1^T & A_3^T\\ A_2^T & A_4^T\end{pmatrix}
	=\begin{pmatrix}A_1A_2^T-A_2A_1^T & A_1A_4^T-A_2 A_3^T\\
	A_3A_2^T-A_4A_1^T & A_3A_4^T-A_4A_3^T\end{pmatrix}.\]
	Thus
	\[A_1A_2^T=A_2A_1^T,\;A_1A_4^T-A_2A_3^T=I.\tag{1}\]
	Also notice that
	\[\begin{pmatrix}I & 0\\-A_3 A_1^{-1} & I\end{pmatrix}
	\begin{pmatrix}A_1 & A_2\\ A_3 & A_4\end{pmatrix}=
	\begin{pmatrix}A_1 & A_2\\ 0 & A_4-A_3A_1^{-1}A_2\end{pmatrix},\]
	thus
	\[\det A=\det\big(A_1(A_4-A_3A_1^{-1}A_2)^T\big).\]
	Easy to know $A_1(A_4-A_3A_1^{-1}A_2)^T=I$ by $(1)$,
	thus finish the proof.
\end{proof}

\section{Rank and Equivalence of Matrices}
\begin{pro}%5
	Prove that any matrix with rank $r$ can be the sum of $r$ matrices with rank $1$.
\end{pro}
\begin{proof}
	Notice that by Theorem $4$(c.f. Page $116$), there are invertible matrices $P,Q$ such that
	\[PAQ=\begin{pmatrix} I_r & 0\\ 0 & 0\end{pmatrix}=\sum_{j=1}^r A_j,\]
	where $\rank(A_j)=1$. Thus
	\[A=\sum_j P^{-1} A_j Q^{-1}\]
	with $\rank(P^{-1} A_j Q^{-1})=\rank(A_j)=1$.
\end{proof}

\begin{pro}%6
	Prove that a $m\times n$ matrix $A$ has rank $1$ if and only if $A=\alpha\beta$, where $\alpha,\beta$ are $m\times 1$ and $1\times n$ non-zero matrices respectively.
\end{pro}
\begin{proof}
	If $\rank(A)=1$, then there is invertible matrices $P,Q$ with order $m,n$ respectively such that
	\[P^{-1}AQ^{-1}=\begin{pmatrix}1 & 0\\ 0 & 0\end{pmatrix}.\]
	Thus
	\[A=P\cdot \begin{pmatrix} 1\\ \vdots\\0\end{pmatrix}\cdot (1\cdots 0)\cdot Q.\]
	Conversely, if $A=\alpha\beta$, then $\rank(A)\leq \rank(\alpha)=1$. Also, if we let
	\[\alpha=(a_1,\dots,a_n)^T,\beta=(b_1,\dots,b_n),\]
	where $a_i\neq 0,b_j\neq 0$, then $A(i,j)=a_ib_j\neq 0$, thus $\rank(A)=1$.
\end{proof}

\begin{pro}%7
	Suppose $A,B\in F^{m\times n}$, then
	\[\rank(A+B)\leq \rank(A)+\rank(B).\]
\end{pro}
\begin{proof}
	let $\rank(A)=r,\rank(B)=s$ and $a_1,\dots,a_r;b_1,\dots,b_s$ are the row vectors of $A, B$ respectively such that they are linear independent respectively, then easy to know all row vectors of $A+B$ can be the linear combinations of $a_1,\dots,a_r;b_1,\dots,b_s$, thus
	\[\rank(A+B)=\rank(\mbox{row vectors of}\;A+B)\leq r+s.\]
	We can give another proof here,
	\[\rank A+\rank B=\rank\begin{pmatrix} A & A+B\\0 & B\end{pmatrix}\geq \rank(A+B).\]
\end{proof}

\begin{pro}%8
	Suppose $A\in\m{R}^{m\times n}$, then
	\[\rank(AA^T)=\rank(A^T A)=\rank(A).\]
\end{pro}
\begin{proof}
	First notice that $\rank(A^T A)\leq \rank(A)$. Suppose column vector $X\in\m{R}^n$ such that
	$A^T AX=0$, then $X^TA^TAX=(AX)^T(AX)=0$, easy to know $AX=0$, thus $\rank(A^T A)\geq \rank(A)$.

	Now we try to prove this using language of matrices, easy to know there is some invertible matrices 
	$P\in\m{R}^{m\times m}$ and $Q\in\m{R}^{n\times n}$ such that if we let 
	\[H_r=\begin{pmatrix}I_r & 0\\0 & 0\end{pmatrix}\in\m{R}^{m\times n},\]
	then $A=PH_rQ$, $\rank(A)=r$ and
	\[\rank(A^T A)=\rank(H_r\cdot P^T P\cdot H_r^T).\tag{1}\]
	Now we write
	\[P^TP=\begin{pmatrix} C_1 & C_2\\ C_2^T & C_3\end{pmatrix}\]
	where $C_1\in\m{R}^{r\times r}$, then we can prove that $\rank(C_1)=r$ as follows.
	By Binet-Cauchy formula we have
	\begin{align*}
	\det C_1&=\sum_{1\leq j_1<j_2<\dots <j_r\leq n} P^T\begin{pmatrix} 1 & 2 &\dots & r\\
	j_1 & j_2 &\dots &j_r\end{pmatrix}\cdot P\begin{pmatrix} j_1 & j_2 &\dots &j_r\\
	1 & 2 &\dots & r\end{pmatrix}\\
	&=\sum_{1\leq j_1<j_2<\dots <j_r\leq n} \bigg(P^T\begin{pmatrix} 1 & 2 &\dots & r\\
	j_1 & j_2 &\dots &j_r\end{pmatrix}\bigg)^2.
	\end{align*}
	Suppose $\det C_1=0$, we know
	\[P^T\begin{pmatrix} 1 & 2 &\dots & r\\j_1 & j_2 &\dots &j_r\end{pmatrix}=0.\]
	Thus by Laplace we know $\det P=0$, which is a contradiction. Thus $\det C_1\neq 0$ or $\rank(C_1)=r$.
	Now try to calculate $(1)$, we know 
	\[H_r\cdot P^T P\cdot H_r^T=\begin{pmatrix} C_1 & 0\\ 0 & 0\end{pmatrix}.\]
	Finally we know that $\rank(A^T A)=\rank(C_1)=r=\rank(A)$.
\end{proof}

\begin{pro}%9
	Suppose an order $n$ square matrix $A$ has the form of
	\[A=\begin{pmatrix} A_1 & A_2\\ A_3 & A_4\end{pmatrix},\]
	where $A_1$ is an invertible matrix with order $1\leq r<n$. Prove that $\rank(A)=r$ if and only if $A_4=A_3 A_1^{-1} A_2$.
\end{pro}
\begin{proof}
	By Schur we have
	\[\begin{pmatrix}I_r & 0\\ -A_3 A_1^{-1} & I_{n-r}\end{pmatrix}
	\begin{pmatrix} A_1 & A_2\\ A_3 & A_4\end{pmatrix}
	=\begin{pmatrix} A_1 & A_2\\ 0 & A_4-A_3 A_1^{-1} A_2\end{pmatrix}.\]
	Assume $A_4=A_3 A_1^{-1} A_2$, easy to know $\rank(A)=r$. Conversely, assume $A_4-A_3 A_1^{-1} A_2\neq 0$, then
	there is some $a\in A_4-A_3 A_1^{-1} A_2$ and $a\neq 0$, thus the sub-matrix
	\[\begin{pmatrix} A_1 & *\\ 0 & a\end{pmatrix}\]
	has non-zero determinant, which leads to that $\rank(A)\geq r+1$, which is a contradiction.
\end{proof}

\begin{pro}%10
	Suppose $A\in F^{n\times n}$, $\rank(A)=r$ and $B\in F^{s\times n}$ is the matrix made by $s$ many rows of $A$
	arbitrarily. Prove that $\rank(B)\geq r+s-n$.
\end{pro}
\begin{proof}
	Without losing generality, we suppose $B$ is made by the first $r$ many rows of $A$. We consider two linear space
	\[W=\{Y\in F^{s\times 1}\colon B^T Y=0\},\;V=\{X\in F^{n\times 1}\colon A^T X=0\}.\]
	Easy to know $\dim W=s-\rank(B)$ and $\dim V=n-r$. Now we prove $\dim W\leq \dim V$.
	Let $Y=(y_1,\dots, y_s)^T\in W$, i.e. $B^TY=0$, and let $\widetilde{Y}=(y_1,\dots,y_s,0,\dots,0)^T\in F^{n\times 1}$,
	then $A^T \widetilde{Y}=0$, i.e. $\widetilde{Y}\in V$. Also, we define the map
	\[\sigma: F^{n\times 1}\to F^{s\times 1},\; (x_1,\dots,x_n)\mapsto (x_1,\dots,x_s)\]
	then we have $Y=\sigma(\widetilde{Y})$ and easy to know $\sigma$ is a linear map.
	Thus if we let $X_1,X_2,\dots,X_t$ is a basis of $V$, then
	\begin{align*}
	Y&=\sigma(\widetilde{Y})\\
	 &=\sigma(k_1X_1+\dots+k_t X_t)\\
	 &=k_1\sigma(X_1)+\dots+k_t \sigma(X_t).
	\end{align*}
	This means $W\subset \Span{\sigma(X_1),\dots,\sigma(X_t)}$, i.e. $\dim W\leq t=\dim V$.
	Finally, $s-\rank(B)\leq n-r$.

	Here we also give a proof based on row vectors. First we prove a lemma
	\begin{lem}
		Suppose $A,B$ are matrices with the same many columns, then 
		\[\rank \begin{pmatrix} A\\ B\end{pmatrix}\leq \rank(A)+\rank(B).\tag{1}\]
	\end{lem}
	\begin{proof}
		Suppose $\rank(A)=n,\rank(B)=m$, with $a_1,\dots,a_n$ and $b_1,\dots,b_m$ are the basic row vectors of rows of $A $ and $B$ respectively, then easy to know if we call $C$ the left matrix in $(1)$, then any row of $C$ can be the linear combination of $a_1,\dots,a_n,b_1,\dots,b_m$. This means $\rank(C)\leq \rank(A)+\rank(B)$. Also, we can give a pure matrix form proof here.
		\[\rank\begin{pmatrix} A & 0\\0 & B\end{pmatrix}= \rank\begin{pmatrix} A & 0\\B & B\end{pmatrix}
		\geq \rank\begin{pmatrix} A\\B\end{pmatrix}.\]
	\end{proof}
	Now back to the original question. Easy to know by lemma, if we let
	\[A=\begin{pmatrix} B\\ C\end{pmatrix},\]
	then $r\leq \rank(B)+\rank(C)\leq \rank(B)+n-s$.
\end{proof}

\begin{pro}%11
	Suppose $A\in F^{m\times n}$, $\rank(A)=r$, $B$ is the sub-matrix of $A$ made by any $s$-many rows and $t$-many columns of $A$. Prove that $\rank(B)\geq r+s+t-m-n$.
\end{pro}
\begin{proof}
	Just let 
	\[A=\begin{pmatrix} B & C\\ D & E\end{pmatrix},\]
	where $B\in F^{s\times t}$. Then
	\[\rank(A)\leq \rank(B,C)+\rank(D,E).\]
\end{proof}

\begin{pro}%12
	Suppose there are at least $n^2-n+1$-many zero in order $n$ square matrix $A$. Prove that $\rank(A)<n$, also try to find the maximum of $\rank(A)$.
\end{pro}
\begin{proof}
	Since there are at most $n-1$-many non-zero elements in $A$, easy to know there are at least one row of $A$ made by all zero, thus $\det(A)=0$. Easy to know the maximum possible value of $\rank(A)$ is $n-1$.
\end{proof}

\begin{pro}%13
	Prove that for order $n$ square matrix $A$,
	\[\rank(A^*)=\begin{cases}
	n, & \mbox{if}\;\rank(A)=n,\\
	1, & \mbox{if}\;\rank(A)=n-1,\\
	0, & \mbox{if}\;\rank(A)<n-1.\end{cases}\]
	Also we know
	\[(A^*)^*=\begin{cases}
	(\det A)^{n-2} A, & \mbox{if}\;n>2,\\
	A, & \mbox{if}\;n=2.\end{cases}\]
\end{pro}
\begin{proof}
	Notice that $A A^*=\det(A)I$, thus $\rank(A)=n$ if and only if $\rank(A^*)=n$. Suppose $\rank(A)=n-1$, then there are at least one invertible sub-matrix of $A$, which means $\rank(A^*)\geq 1$. Consider the equation $AX=0$, easy to know the dimension of its solution space is $n-\rank(A)=1$, this means $\rank(A^*)\leq 1$, thus we know $\rank(A^*)=1$.
	Suppose $\rank(A)<n-1$, easy to know $A^*=0$, i.e. $\rank(A^*)=0$. As for the form of $(A^*)^*$, easy to know if $\rank(A)=n\geq 2$, then $(A^*)^*=(\det A)^{n-1}A$. If $n>2$ and $\det A=0$, then from what we have proved above we know that $\rank(A^*)\leq 1<n-1$, means $\rank(A^*)^*=0$, i.e. $(A^*)^*=0$. If $n=2$, then just let $A=(a,b;c,d)$, easy to know $(A^*)^*=A$.
\end{proof}

\begin{pro}%14
	Suppose $A, B$ are matrices with the same many rows, prove that $\rank(A,B)\leq \rank(A)+\rank(B)$.
\end{pro}
\begin{proof}
	Similar like the lemma in problem $3.4.10$.
\end{proof}

\begin{pro}%15
	Suppose $A$ is a $m\times n$ matrix with integer elements (called integer matrix below). Prove that there are some invertible integer matrix $P$ and $Q$ with order $m$ and $n$ respectively. (Here invertible means its regular inverse matrix with real numbers is also integer matrix), such that
	\[A=P\begin{pmatrix}\diag(d_1,d_2,\dots,d_r) & 0\\ 0 & 0\end{pmatrix}_{m\times n}Q,\]
	where $d_1,d_2,\dots,d_r\in \m{N}^+$ and $d_i\mid d_{i+1},\;i=1,2,\dots,r-1$.
\end{pro}
\begin{proof}
	Similar to the proof of Theorem $2$(c.f. Textbook Page 279).
\end{proof}

\section{Examples}
\begin{pro}%1
	Given any matrices $A\in F^{m\times n}\, B\in F^{n\times m}$, then $\rank(A)=\rank (AB)$ if and only if there is some $C\in F^{m\times n}$ such that $A=ABC$.
\end{pro}
\begin{proof}
	If $A=ABC$, then $\rank(A)=\rank(ABC)\leq \rank (AB)\leq \rank (A)$, thus $\rank (A)=\rank (AB)$. Conversely, assume $\rank (A)=\rank(AB)=r\geq 1$, then there are some invertible matrices $P$ and $Q$ such that 
	\[A=P\Hermit{r}Q.\]
	Thus we know $ABC=A$ if and only if 
	\[\Hermit{r}=\Hermit{r}QBCQ^{-1},\tag{1}\]
	and now we only need to construct $CQ^{-1}$. Let 
	\[D=QB=\begin{pmatrix} D_1 & D_2\\ D_3 & D_4\end{pmatrix},\]
	where $D_1\in F^{r\times r}$, then since  
	\[\Hermit{r}\begin{pmatrix} D_1 & D_2\\ D_3 & D_4\end{pmatrix}=
	\begin{pmatrix} D_1 & D_2\\ 0 & 0\end{pmatrix},\] 
	one can derive $\rank (D_1,D_2)=r$. This means there are $r$-many columns of $(D_1,D_2)$ which will construct an invertible order-$r$ sub-matrix, thus there is some invertible order-$n$ matrix $R$, such that 
	\[\begin{pmatrix} D_1 & D_2\\ 0 & 0\end{pmatrix}R=\begin{pmatrix} D_1' & D_2'\\ 0 & 0\end{pmatrix},\]
	where $\rank(D_1')=r$, and thus 
	\[\Hermit{r}DR\begin{pmatrix}D_1'^{-1} & 0\\ 0 & 0 \end{pmatrix}=\Hermit{r}.\]
	Now we only need to let 
	\[C=R\begin{pmatrix}D_1'^{-1} & 0\\ 0 & 0 \end{pmatrix} Q,\]
	then by some easy multiplication one can easily derive $(1)$.
\end{proof}

\section{Linear Equations}
\begin{pro}%9
	Suppose $A,B$ are order $n$ squared matrices on $F$ with ranks $r$ and $n-r$ respectively, find all solutions of 
	\[AXB=0.\tag{1}\]
\end{pro}
\begin{proof}
	Since $\rank A=r,\rank B=n-r$, we let
	\[A=P_1\Hermit{r} Q_1,\quad B=P_2\Hermit{n-r} Q_2,\]
	where $P_1,P_2,Q_1,Q_2$ are all invertible fixed matrices. Then (1) is true if and only if
	\[\Hermit{r}Q_1XP_2\Hermit{n-r}=0.\tag{2}\]
	Now let 
	\[Q_1XP_2=\begin{pmatrix} C_{r\times (n-r)}& D\\ E& G\end{pmatrix},\]
	and put it into (2), we know (2) is true if and only if $C=0$, i.e.,
	\[X=Q_1^{-1}\begin{pmatrix} 0_{r\times (n-r)}& D\\ E& G\end{pmatrix}P_2^{-1},\]
	where $D\in F^{r\times r},\; E\in F^{(n-r)\times (n-r)}$ and $G\in F^{(n-r)\times r}$ are arbitrary matrices.
\end{proof}
\section{Generalized Inverses of Matrices}
\begin{pro}%1
	Suppose $A\in F^{m\times n}$ and $B\in F^{m\times p}$, then the equation $AX=B$ has solutions if and only if $B=AA^- B$. Moreover, when it has solutions, $X$ is a solution if and only if
	\[X=A^-B+(I_n-A^-A)W,\]
	where $W\in F^{n\times p}$ is an arbitrary matrix.
\end{pro}
\begin{proof}
	If $B=AA^-B$, surely $AX=B$ has a solution $A^-B$, conversely, suppose $x_0$ is a solution, then
	\[B=Ax_0=AA^-A x_0=AA^- B.\tag{1}\]
	Moreover, $A^-B+(I_n-A^-A)x_0=A^-B+x_0+A^-Ax_0=x_0$. Now suppose $W\in  F^{n\times p}$ is an arbitrary matrix and $AX=B$ does has a solution, then (1) is true and thus
	\begin{align*}
	A\big(A^-B+(I_n-A^-A)W\big)&=AA^-B+AW-AA^-AW\\
							   &=AA^-B+AW-AW\\
							   &=B,
	\end{align*}
	which completes our proof.
\end{proof}

\begin{pro}%2
	Suppose $A\in F^{m\times n},\; B\in F^{p\times q}$ and $C\in F^{m\times q}$, then $AXB=C$ has solutions if and only if $(I_m-AA^-)C=0$ and $C(I_q-B^-B)=0$. Moreover, when it does has solutions, $X$ is a solution if and only if 
	\[X=A^-CB^-+(I_n-A^-A)Y+Z(I_p-BB^-)+(I_n-A^-A)W(I_p-BB^-),\tag{1}\]
	where $Y,Z$ and $W$ are arbitrary matrices in $F^{n\times p}$.
\end{pro}
\begin{proof}
	Suppose $Ax_0B=C$, then 
	\begin{align*}
		C&=Ax_0B=AA^- Ax_0B=AA^- C\\
		 &=Ax_0B=Ax_0BB^- B=CB^- B.
	\end{align*}
	Now suppose $C=AA^- C=CB^-B$, then $C=AA^-C=AA^-CB^-B$, which means $AXB=C$ has a solution $A^-CB^-$. Moreover, if (1) is true, easy to verify $AXB=C$. Conversely, if $Ax_0B=C$ and let $Y=x_0BB^-,Z=x_0$ and $W=0$, easy to verify (1) is also true if we let $X=x_0$, which completes our proof.
\end{proof}

\begin{pro}%3
	Suppose $A\in F^{m\times p},\;B\in F^{q\times n}$ and $C\in F^{m\times n}$, then $AX-YB=C$ has solutions if and only if 
	\[(I_m-AA^-)C(I_n-B^-B)=0.\tag{1}\]
	Moreover, when it has solutions, $(X,Y)$ is a solution if and only if 
	\begin{align*}
		X&=A^- C+A^- ZB+(I_p-A^-A)W,\\
		Y&=-(I_m-AA^-)CB^-+Z-(I_m-AA^-)ZBB^-,\tag{2}
	\end{align*}
	where $W\in F^{p\times n}$ and $Z\in F^{m\times q}$ are arbitrary matrices.
\end{pro}
\begin{proof}
	If $(x_0,y_0)$ is a solution to $AX-YB=C$, then easy to verify (1) is true by replacing $C$ by $Ax_0-y_0B$. Conversely, if (1) is true, then \[C=AA^-C(I-B^-B)+CB^-B,\] which means $\big(A^-C(I-B^-B),-CB^-\big)$ is a solution. Moreover, if (2) is true, then $(X,Y)$ is indeed a solution, and conversely, if $Ax_0-y_0B=C$, we can get (2) by assuming $X=W=x_0$ and $Y=Z=y_0$.
\end{proof}
	
\begin{pro}%5
	Suppose $A$ is any complex matrix, then $\overline{(A^+)}^T=(\overline{A}^T)^+$.
\end{pro}
\begin{proof}
	For any complex matrix $B$, define $B^*$ by $\overline{B}^T$, then $(B_1B_2)^*=B_2^* B_1^*$. Since
	\begin{align*}
		A^*(A^+)^*A^*&=(AA^+A)^*=A^*,\\
		(A^+)^*A^*(A^+)^*&=(A^+AA^+)^*=A^*,\\
		\big(A^*(A^+)^*\big)^*&=A^+A=(A^+A)^*=A^*(A^+)^*,\\
		\big((A^+)^*A^*\big)^*&=AA^+=(AA^+)^*=(A^+)^*A^*,
	\end{align*}
	we know $(A^+)^*$ is the Moore-Penrose generalized inverse of $A^*$, i.e., $(A^+)^*=(A^*)^+$ by Theorem $2$ (cf. Textbook Page 148). Using the similar way, we can show that $(A^+)^+=A$ and $(\la A)^+=\la^+ A^+$ for any $\la\in F$ and $A\in F^{m\times n}$, where $\la^+=\la^{-1}$ if $\la\neq 0$ and $0^+=0$.
\end{proof}