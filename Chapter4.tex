\chapter{Linear Spaces}
\section{Definition of Linear Spaces}

\section{Linear Dependence}
\begin{pro}%5
	Suppose $a_1,a_2,\dots,a_k\in F^n$ is linear dependent, $k\geq 2$. Prove that for any $a_{k+1}\in F^n$, there are not-all-zero scalars $\la_1,\la_2,\dots,\la_k\in F$ such that $a_1+\la_1a_{k+1},a_2+\la_2a_{k+1},\dots,a_k+\la_k a_{k+1}$ is linear dependent.
\end{pro}
\begin{proof}
	There are not-all-zero scalars $\mu_1,\dots,\mu_k$ such that $\mu_1 a_1+\dots+\mu_k a_k=0$. Thus we know
	\[\mu_1 (a_1+\la_1 a_{k+1})+\dots+\mu_k (a_k+\la_k a_{k+1})=\mu_1\la_1+\dots+\mu_k\la_k.\tag{1}\]
	Notice that $\rank(\mu_1,\dots,\mu_k)=1$, thus $\la$-s can be not-all-zero to make $(1)$ to be zero.
\end{proof}

\begin{pro}%8
	Suppose $(V,\m{R})$ is the linear space of all continuous functions on real line. Prove that 
	\[\{\sin x,\sin 2x,\dots,\sin nx,\dots\}\tag{1}\]
	is linear independent.
\end{pro}
\begin{proof}
	For any integer sequence $1\leq n_1<n_2<\dots<n_r$, suppose 
	\[k_1\sin(n_1x)+\dots k_r\sin(n_r x)=0.\]
	Then easy to know that if we let $K=(k_1,k_2,\dots, k_r)$, then $AK^T=0$, where
	\[A=\begin{pmatrix}
	n_1 & \cdots & n_r\\
	n_1^3 & \cdots & n_r^3\\
	\vdots&  &\vdots\\
	n_1^{2r-1} & \cdots & n_r^{2r-1}\end{pmatrix}.\]
	Since 
	\[\det A=n_1\cdots n_r\prod_{1\leq j<k\leq r}(n_k^2-n_j^2)\neq 0,\]
	we know $K\neq 0$, which means the set $(1)$ is linear independent. Similarly, we know 
	\[\{1,\cos x,\cos 2x,\dots,\cos nx,\dots\}\]
	is also linear independent.
\end{proof}

\begin{pro}%9
	Suppose the $t$-many $n$-order row vectors $\alpha_i=(a_{i1},a_{i2},\dots,a_{in}),\;i=1,2,\dots,t\leq n$ have the property that $2|a_{ii}|>\sum_{k=1}^n |a_{ik}|$ for all $1\leq i\leq t$. Prove that $S=\{\alpha i:q\leq i\leq t\}$ is linear independent.
\end{pro}

\begin{pro}%10
	Suppose $(V,F)$ is a linear space and vectors $a_1,a_2,\dots,a_n$ is linear independent. Prove that for any vector $b\in V$, there are at most one vector in $b,a_1,\dots,a_n$ such that it can be the linear combination of all vectors before it.
\end{pro}
\begin{proof}
	Suppose there are two such vectors, say they are $a_j$ and $a_k$, then easy to know that there are some none-zero scalars $c_1,c_2$ such that
	\[a_j=c_1b+*,\quad a_k=c_2b+*,\]
	where $*$ represents any linear combination of $a_1,\dots,a_n$. Thus easy to know $c_2 a_j-c_1 a_k=*$, which is a contradiction.
\end{proof}

\begin{pro}%12
	$A$ is an order $n$ square matrix, prove that
	\[\rank A^n=\rank A^{n+1}=\rank A^{n+2}=\dots.\]
\end{pro}
\begin{proof}
	Let $r=\rank A$, notice that $0\leq r\leq n$. Now for every step by times $A$ for $A^k$, we know that $\rank(A^{k+1})\leq \rank(A^k)$, suppose there is some $1\leq k\leq n$, such that $\rank(A^{k+1}=\rank(A^k)$, then by problem $3.5.1$ we know the prove is already done. Now suppose for all $1\leq k\leq n$, such relation is strictly $<$, then easy to know we must have $\rank(A^n)=0$, i.e. $A^n=0$. In this case, the proof also is done now.
\end{proof}

\begin{pro}
	Suppose all $f_1,f_2,\dots,f_n$ have $n$-th derivatives at $\m{R}$ and there is some $x_0\in \m{R}$ such that 
	\[W(f_1,\dots,f_n;x_0)\neq 0,\]
	where $W$ is the Wronskian Determinant, then $\{f_1,\dots,f_n\}$ is linear independence. Now suppose for all $x\in[0,+\infty)$,
	\[W(f_1,\dots,f_n;x)=0,\]
	is $\{f_1,\dots,f_n\}$ linear dependent?
\end{pro}
\begin{figure}
	\centering
	\includegraphics{Wronski.pdf}
\end{figure}
\begin{proof}
	Not really. We define $f\in C^{\infty}[0,+\infty)$ by
	\[f(x)=
	\begin{cases}
		\exp\Big(-\tan^2\big(\pi(x-\frac{1}{2})\big)\Big), & x\notin \m{N},\\
		0,& x\in \m{N},
	\end{cases}
	\]  
	then
	\[f^{(n)}(m)=0,\quad \forall n,m\in \m{N}\]
	and $f$ is a periodic function with period $1$. Now for $n\in\m{N}^+$ let 
	\[f_n(x)=\frac{1}{n^m} f(x),\quad m\leq x<m+1,\, m\in\m{N},\]
	then $f_n\in C^{\infty}[0,+\infty)$ for all $n$. One can easily verify that 
	for any $k\geq 2$ and $1\leq j_1<j_2<\dots<j_k$, $\{f_{j_1},\dots f_{j_k}\}$ is linear independence but
	\[W(f_{j_1},\dots,f_{j_k};x)\equiv 0,\quad \forall x\in [0,+\infty).\]
	If a finite family of analytic functions has an identically zero Wronskian, then it is linearly dependent. one can find a simple proof in~\cite{Wronskian2010}. Also a free version is available at \url{https://arxiv.org/abs/1301.6598}.
\end{proof}

\section{Basis and Coordinates}
\begin{pro}%4
	Find a basis $\{A_1,A_2,A_3,A_4\}$ for the space $(F^{2\times 2},F)$, such that for all $j$, we have $A_j^2=A_j$.
\end{pro}
\begin{proof}
	By Problem $3.1.10$, easy to know that
	\[E_{11},E_{22},E_{11}+E_{21},E_{11}+E_{12}\]
	is a basis suitable here.
\end{proof}

\begin{pro}%5
	Prove that in $F_{n+1}[x]$, $A=\{1,x+a,(x+a)^2,\dots, (x+a)^n\}$ is a basis, for all $a\in F$. Then find the coordinates under this basis of vector $f(x)=a_0+a_1x+\dots+a_n x^n$.
\end{pro}
\begin{proof}
	Easy to know for all $k\leq n$, $\max_k \{\deg (x+a)^n\}=n$, thus $A$ is linear independent. Since $|A|=n+1=\dim F_{n+1}[x]$, we know $A$ is a basis. Easy to know that for $0\leq k\leq n$, we have
	\[f^{(k)}(x)=k!\sum_{m=k}^n \binom{m}{k} a_m x^{m-k}.\]
	Thus we know that
	\begin{align*}
		f(x)&=\sum_{k=0}^n \frac{f^{(k)}(-a)}{k!} (x+a)^k\\
			&=\sum_{k=0}^n \bigg(\sum_{m=k}^n \binom{m}{k} a_m (-a)^{m-k}\bigg) (x+a)^k.
	\end{align*}
\end{proof}

\section{Basis and Coordinates Transform}
\begin{pro}%2
	Since $(F_{n+1}[\cos x])$ is a linear space, easy to know that both $A=\{1,\cos x,\dots, \cos nx\}$ and $B=\{1,\cos x,\dots, \cos^n x\}$ are its basis. Try to find the transition matrix $T$ from $A$ to $B$.
\end{pro}
\begin{proof}
	First we need to find the value of
	\[I(n,m)=\int_0^{2\pi} \cos nx \cos mx\,dx,\quad (n,m\in \m{N}).\]
	We give two ways here.
	\begin{description}
	\item[(a)] for $n=m=0$, easy to know $I(0,0)=2\pi$ and for $n=m\geq 1$, $I(n,n)=\pi$. Now suppose $n>m\geq 0$. 
	Since 
	\[\int_0^{2\pi} \cos nx \cos mx\,dx
	\pm \int_0^{2\pi} \sin nx \sin mx\,dx=
	\int_0^{2\pi} \cos (n\mp m)x \,dx=0,\]
	we know $I(n,m)=0$.
	\item[(b)] Easy to know that
	\begin{align*}
		I(n,m)&=\frac{1}{4}\int_0^{2\pi} (e^{inx}+e^{-inx})(e^{imx}+e^{-imx})\, dx\\
		&=\frac{1}{2}\int_0^{2\pi} (\cos(n+m)x +\cos(n-m) x)\,dx.\\
	\end{align*}
	\end{description}
	Anyway, we know that
	\[I(n,m)=\begin{cases}
	2\pi, &\mbox{if}\;n=m=0,\\
	\pi, &\mbox{if}\; n=m\geq 1,\\
	0, &\mbox{if}\; n\neq m.\end{cases}\]
	Now we need to calculate 
	\[J(n,m)=\int_0^{2\pi} \cos^n x \cos mx\,dx,\quad (n,m\in \m{N}).\]
	By notice that $\cos x=(e^{ix}+e^{-ix})/2$, easy to find out that
	\[J(n,m)=\begin{cases}
	\dfrac{2\pi}{2^n} \dbinom{n}{(n-m)/2}, &\mbox{if}\; n\geq m\;\mbox{and}\; 2\mid (n-m),\\
	0, &\mbox{others}.\end{cases}\]
	Thus if we let 
	\[\cos^j x=a_{j0}+a_{j1}\cos x+a_{j2}\cos 2x+\dots +a_{jn} \cos nx,\quad(0\leq j\leq n),\]
	we derive that for $0\leq j,k\leq n$
	\[a_{jk}=\begin{cases}
	J(j,k)/(2\pi), &\mbox{if}\; k=0\\
	J(j,k)/\pi, &\mbox{if}\; k\geq 1.\end{cases}\]
	Actually we can give a brief form of $T$ by
	\[T=\begin{pmatrix}
	1& & & & &*\\
	 &1& & & & \\
	 & &\frac{1}{2}& & &\\
	 & & &\frac{1}{2^2}& & \\
	 & & & &\ddots&\\
	0& & & & &\frac{1}{2^{n-1}}
	\end{pmatrix}\]
	Thus $\det T=2^{-n(n-1)/2}$ and $\tr T=3-\frac{1}{2^{n-1}}$.
\end{proof}

\begin{pro}%4
	Under the standard basis $(\ep_1,\ep_2,\ep_3,\ep_4)$ of $\m{R}^4$, we know that the unit ball has the function
	\[F(x)=x_1^2+x_2^2+x_3^2+x_4^2=1.\]
	Now let $a_1=(1,1,1,1),a_2=(1,1,-1,-1),a_3=(1,-1,1,-1),a_4=(1,-1,-1,1)$. Find that function of unit ball under this new basis $\{a_1,a_2,a_3,a_4\}$.
\end{pro}
\begin{proof}
	Let the coordinates under the new basis is $y=(y_1,y_2,y_3,y_4)^T$ and the transition matrix from old to new is $A$, then easy to know $x=Ay$. Thus the new function is
	\[G(y)=F(Ay)=1.\]
\end{proof}

\section{Isomorphism}
\begin{pro}%1
	Show that $\m{R}$ as a real linear space is isomorphic to the real linear space $\m{R}^+$ in Example $4.1.2$.
\end{pro}
\begin{proof}
	Easy to know that $\exp(\cdot):\m{R}\to\m{R}^+$ is an isomorphism.
\end{proof}

\begin{pro}%2
	If there is an bijection between two vector space $V_1$ and $V_2$ on $\m{Q}$, is $V_1$ isomorphic to $V_2$?
\end{pro}
\begin{proof}
	No. For example, we let $V_1=\m{Q}$ where the addition(ad.) and scaler multiplication(sm.) are standard ones respectively, then easy to know $V_1=\Span{1}$ and $\dim V_1=1$. Let 
	\[V_2=\{a+b\sqrt{2}\colon a,b\in\m{Q}\}\]
	where the ad. and sm. are standard ones too, then $V_2=\Span{1,\sqrt{2}}$ and $\dim V_2=2$. Now since both $V_1$ and $V_2$ are both countable sets, i.e. $|V_1|=|V_2|=|\m{N}|$, there must be an bijection between them, but as above we know that $\dim V_1<\dim V_2$, thus they are not isomorphic to each other.
\end{proof}

\begin{pro}%3
	Suppose $V$ is an $n$-dim complex vector space. Now let $V^-=(V,\m{R})$ where the ad. is the ad. in $V$ and sm. is the restriction of original sm. Try to find $\dim V^-$.
\end{pro}
\begin{proof}
	Easy to know $\dim V^-=2n$. Since $\dim V=n$, we suppose $a_1,a_2,\dots,a_n$ is a basis of $V$, then we say
	\[a_1,a_2,\dots,a_n,ia_1,ia_2,\dots,ia_n\]
	is a basis of $V^-$. Given any $x_1,\dots,x_n,y_1,\dots,y_n\in\m{R}$ with
	\[x_1a_1+\dots +x_na_n+y_1(ia_1)+\dots +y_n(ia_n)=0\]
	easy to know $k_1a_1+\dots k_na_n=0$ where $k_j=x_j+iy_j\in \m{C}$, then $k_j=0$ and thus $x_j=y_j=0$ for all $j$. 
	Given any vector in $V$ with the form 
	\[a=k_1a_1+\dots +k_na_n,\quad k_j\in\m{C},\]
	just let $x_j=\Re(k_j)$ and $y_j=\Im(k_j)$, then easy to know
	\[a=x_1a_1+\dots +x_na_n+y_1(ia_1)+\dots +y_n(ia_n).\]
	Finally we know that the statement above is true and thus $\dim V^-=2n$.
\end{proof}

\begin{pro}%4
	Suppose $V$ is an $n$-dim real vector space. Now let $V^-=(V,\m{Q})$ where the ad. is the ad. in $V$ and sm. is the restriction of original sm. Does $V^-$ must be a finite dimensional space?
\end{pro}
\begin{proof}
	Not really, see problem $4.3.6$.
\end{proof}

\section{Subspaces}
\begin{pro}%1
	Suppose $W=\{A\in F^{n\times n}\colon \tr(A)=0\}$, prove that $W$ is a subspace of $F^{n\times n}$ and try to find 
	$\dim W$.
\end{pro}
\begin{proof}
	Since $\tr(A+B)=\tr(A)+\tr(B)$, thus $W$ is indeed a subspace. Let $A_i\;(2\leq i\leq n)$ be a matrix like 
	\[\begin{pmatrix}
	1 & & & \\
	  &\ddots & & \\
	  & &-1& \\
	  & &  & \ddots
	 \end{pmatrix},\]
	here $-1$ is on the $i$-th row and $i$-th column. Then for a matrix $A\in W$ in form with 
	\[A=\begin{pmatrix}
	a_1 & & \\
	    &\ddots & \\
	    & & a_n\end{pmatrix},\]
	it can be the additive of $A_i$ above,
	\[A=-a_2A_2-\dots-a_nA_n\]
	since $-(a_2+\dots +a_n)=a_1$. Easy to know $A_2,\dots, A_n$ is indeed linear independent. Now for any $A\in W$, we have 
	\[A=\sum_{1\leq i\neq j\leq n} a_{ij} E_{ij}-\sum_{i=2}^n a_{ii} A_i,\]
	and also $(E_{ij},A_i)$ is linear independent, thus $\dim W=n^2-n+(n-1)=n^2-1$.
\end{proof} 

\begin{pro}%2
	Let $V=F^{n\times n}$ and
	\begin{align*}
	S&=\{A\in V\colon A=A^T\},\\
	K&=\{A\in V\colon A=-A^T\}.
	\end{align*}
	Show that $S,K$ are both subspaces and $S+K=V,S\cap K=\{0\}$ and then find out $\dim S$ and $\dim K$.
\end{pro}
\begin{proof}
	Notice that $(A+B)^T=A^T+B^T$, we know both $S$ and $K$ are subspaces. Given any matrix $A\in V$, since 
	\[A=\frac{A+A^T}{2}+\frac{A-A^T}{2},\]
	we know $A\in S+K$, thus $V=S+K$. Given any $A\in S,A\in K$, then $A=0$ since $A=A^T=-A$. Given $A\in S$, it can be expanded as 
	\[A=\sum_{i<j} a_{ij}E_{ij}+\sum_i a_{ii} E_{ii}.\]
	Thus $\dim S=n(n+1)/2$. Given any $A\in K$, we have
	\[A=\sum_{i<j} a_{ij} (E_{ij}-E_{ji})\]
	thus $\dim K=n(n-1)/2$.
\end{proof}

\begin{pro}%3
	Let $V_1=\{(x,-x;y,z)\colon x,y,z\in F\}$ and $V_2=\{(x,y;-x,z)\colon x,y,z\in F\}$. Show that $V_1, V_2$ are subspaces of $F^{2\times 2}$ and finger out $\dim V_1$, $\dim V_2$, $\dim (V_1+V_2)$ and $\dim (V_1\cap V_2)$.
\end{pro}
\begin{proof}
	Notice that 
	\begin{align*}
		V_1&=\Span{E_{11}-E_{12},E_{21},E_{22}},\\
		V_2&=\Span{E_{11}-E_{21},E_{12},E_{22}},\\
		V_1\cap V_2&=\Span{E_{11}-E_{12}-E_{21},E_{22}}.
	\end{align*}
	Thus $\dim V_1=\dim V_2=3$, $\dim (V_1\cap V_2)=2$ and finally $\dim(V_1+V_2)=4$. Actually we have $V_1+V_2=F^{2\times 2}$.
\end{proof}

\begin{pro}%4
	Let
	\begin{align*}
	W&=\{f\in F[x]\colon f(x)=f(-x)\},\\
	U&=\{f\in F[x]\colon f(x)=-f(-x)\}.
	\end{align*}
	Show that $W+U=F[x]$ and $W\cap U=\{0\}$.
\end{pro}
\begin{proof}
	Similar to problem $4.6.2$.
\end{proof}

